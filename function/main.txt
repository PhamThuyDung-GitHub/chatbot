import streamlit as st
import pandas as pd
import os
from sentence_transformers import SentenceTransformer
import chromadb
import google.generativeai as genai
from IPython.display import Markdown
from chunking import SemanticChunker
from utils import process_batch, divide_dataframe, clean_collection_name
from search import vector_search, hyde_search
from llms.onlinellms import OnlineLLMs
import time
from constant import VI, USER, ASSISTANT, VIETNAMESE, ONLINE_LLM, GEMINI, DB
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import Document as langchainDocument
from collection_management import list_collection
from preprocessing import preprocess_text, remove_duplicate_rows  # Import c√°c h√†m c·∫ßn thi·∫øt
# --- Streamlit Configuration ---
st.set_page_config(page_title="UIT Admissions Chatbot", layout="wide" , page_icon="https://tuyensinh.uit.edu.vn/sites/default/files/uploads/images/uit_footer.png")
st.markdown("""
    <style>
        .reportview-container { margin-top: -2em; }
        #MainMenu {visibility: hidden;}
        .stDeployButton {display:none;}
        footer {visibility: hidden;}
        #stDecoration {display:none;}
    </style>
""", unsafe_allow_html=True)

# --- Helper Functions ---
def clear_session_state():
    for key in st.session_state.keys():
        del st.session_state[key]
st.sidebar.header("ƒê√°nh gi√° Chatbot")
review = st.sidebar.text_area("Vi·∫øt ƒë√°nh gi√° c·ªßa b·∫°n t·∫°i ƒë√¢y:")

if st.sidebar.button("L∆∞u ƒë√°nh gi√°"):
    if review.strip():
        with open("user_reviews.txt", "a", encoding="utf-8") as review_file:
            review_file.write(f" Rview of user: {review}\n")
        st.sidebar.success("ƒê√°nh gi√° c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c l∆∞u!")
    else:
        st.sidebar.error("Vui l√≤ng nh·∫≠p n·ªôi dung ƒë√°nh gi√° tr∆∞·ªõc khi l∆∞u!")
# --- UI Setup ---
st.sidebar.title("Doc Retrievaled")
st.markdown(
    """
    <h1 style='display: flex; align-items: center;'>
        <img src="https://tuyensinh.uit.edu.vn/sites/default/files/uploads/images/uit_footer.png" width="50" style='margin-right: 10px'>
        UIT Admissions Chatbot 
    </h1>
    """,
    unsafe_allow_html=True
)
st.markdown("Welcome to the UIT Admissions Chatbot!‚ùì‚ùì‚ùì Discover all the information you need about admissions, üìöprograms, üí∏scholarships, üåüStudent Life at UIT and more with us.")

# --- Session State Initialization ---
# Initialize language first
if "language" not in st.session_state:
    st.session_state.language = VIETNAMESE

# Initialize embedding_model based on language
if "embedding_model" not in st.session_state:
    if st.session_state.language == VI:
        st.session_state.embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')
        st.session_state.embedding_model_name = 'keepitreal/vietnamese-sbert'
    else:
        st.session_state.embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') # Or your preferred multilingual model
        st.session_state.embedding_model_name = 'sentence-transformers/all-mpnet-base-v2'

# Initialize llm_model
if "llm_model" not in st.session_state:
    api_key = "AIzaSyBwvb8_AXyn_IZwn92WnqWQpKKM9sMfszQ"  # Replace with your actual API key
    st.session_state.llm_model = OnlineLLMs(name=GEMINI, api_key=api_key, model_version="learnlm-1.5-pro-experimental")
    st.session_state.api_key_saved = True
    print(" API Key saved successfully!")

# Initialize other session state variables
if "client" not in st.session_state:
    st.session_state.client = chromadb.PersistentClient("db")
if "collection" not in st.session_state:
    st.session_state.collection = None
if "search_option" not in st.session_state:
    st.session_state.search_option = "Hyde Search"
if "open_dialog" not in st.session_state:
    st.session_state.open_dialog = None
if "source_data" not in st.session_state:
    st.session_state.source_data = "UPLOAD"
if "chunks_df" not in st.session_state:
    st.session_state.chunks_df = pd.DataFrame()
if "random_collection_name" not in st.session_state:
    st.session_state.random_collection_name = None

st.session_state.chunkOption = "SemanticChunker"
st.session_state.number_docs_retrieval = 15
st.session_state.llm_type = ONLINE_LLM

# --- Data Upload and Processing ---
st.header("1. Setup data source")
st.subheader("1.1. Upload data (Upload CSV files)", divider=True)
uploaded_files = st.file_uploader("", accept_multiple_files=True)

all_data = []
if uploaded_files:
    for file in uploaded_files:
        if file.name.endswith(".csv"):
            try:
                df = pd.read_csv(file)
                all_data.append(df)
            except pd.errors.ParserError:
                st.error(f"Error: The file {file.name} is not a valid .csv file.")
    df = pd.concat(all_data, ignore_index=True)
    # --- Preprocessing ---
    if not df.empty:
        # √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho c·ªôt "C√¢u h·ªèi" v√† "C√¢u tr·∫£ l·ªùi"
        df["C√¢u h·ªèi"] = df["C√¢u h·ªèi"].apply(preprocess_text)
        df["C√¢u tr·∫£ l·ªùi"] = df["C√¢u tr·∫£ l·ªùi"].apply(preprocess_text)

        # Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p d·ª±a tr√™n c·ªôt "C√¢u h·ªèi"
        df = remove_duplicate_rows(df, "C√¢u h·ªèi")

        # Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p d·ª±a tr√™n c·ªôt "C√¢u tr·∫£ l·ªùi"
        df = remove_duplicate_rows(df, "C√¢u tr·∫£ l·ªùi")
if all_data:
    df = pd.concat(all_data, ignore_index=True)
    
    st.dataframe(df)
    st.subheader("Chunking")

    if not df.empty:
        index_column = "C√¢u tr·∫£ l·ªùi"
        st.write(f"Selected column for indexing: {index_column}")

        chunk_records = []
        for _, row in df.iterrows():
            selected_column_value = row[index_column]
            if isinstance(selected_column_value, str) and selected_column_value:
                chunker = SemanticChunker(embedding_type="tfidf")
                chunks = chunker.split_text(selected_column_value)
                for chunk in chunks:
                    chunk_records.append({**row.to_dict(), 'chunk': chunk})

        st.session_state.chunks_df = pd.DataFrame(chunk_records)

if "chunks_df" in st.session_state and not st.session_state.chunks_df.empty:
    st.write("Number of chunks:", len(st.session_state.chunks_df))
    st.dataframe(st.session_state.chunks_df)

# --- Data Saving ---
if st.button("Save Data"):
    if st.session_state.chunks_df.empty:
        st.warning("No data available to process.")
    else:
        try:
            if st.session_state.collection is None:
                collection_name = "rag_collection"
                if uploaded_files:
                    collection_name = f"rag_collection_{clean_collection_name(os.path.splitext(uploaded_files[0].name)[0])}"
                st.session_state.random_collection_name = collection_name
                st.session_state.collection = st.session_state.client.get_or_create_collection(
                    name=collection_name,
                    metadata={"Chunk ": "", "Question": "", "Answer": ""}
                )

            batch_size = 256
            df_batches = divide_dataframe(st.session_state.chunks_df, batch_size)
            num_batches = len(df_batches)

            progress_text = "Saving data to Chroma. Please wait..."
            my_bar = st.progress(0, text=progress_text)

            for i, batch_df in enumerate(df_batches):
                if not batch_df.empty:
                    process_batch(batch_df, st.session_state.embedding_model, st.session_state.collection)
                    progress_percentage = int(((i + 1) / num_batches) * 100)
                    my_bar.progress(progress_percentage, text=f"Processing batch {i + 1}/{num_batches}")
                    time.sleep(0.1)

            my_bar.empty()
            st.success("Data saved to Chroma vector store successfully!")
            st.markdown(f"Collection name: {st.session_state.random_collection_name}")
            st.session_state.data_saved_success = True

        except Exception as e:
            st.error(f"Error saving data to Chroma: {e}")

# --- Load from Saved Collection ---
st.subheader("1.2. Or load from saved collection", divider=True)
if st.button("Load from saved collection"):
    st.session_state.open_dialog = "LIST_COLLECTION"

    def load_func(collection_name):
        st.session_state.collection = st.session_state.client.get_collection(name=collection_name)
        st.session_state.random_collection_name = collection_name
        st.session_state.data_saved_success = True
        st.session_state.source_data = DB
        data = st.session_state.collection.get(include=["documents", "metadatas"])
        metadatas = data["metadatas"]
        column_names = []
        if metadatas and metadatas[0].keys():
            column_names.extend(metadatas[0].keys())
            column_names = list(set(column_names))
        st.session_state.chunks_df = pd.DataFrame(metadatas, columns=column_names)

    def delete_func(collection_name):
        st.session_state.client.delete_collection(name=collection_name)

    list_collection(st.session_state, load_func, delete_func)

if "random_collection_name" in st.session_state and st.session_state.random_collection_name and not st.session_state.chunks_df.empty:
    st.session_state.columns_to_answer = [col for col in st.session_state.chunks_df.columns if col != "chunk"]

# --- Search Algorithm Setup ---
st.header("2. Set up search algorithms")
st.radio(
    "Please select one of the options below.",
    ["Hyde Search", "Vector Search"],
    captions=["Search using the HYDE algorithm", "Search using vector similarity"],
    key="search_option",
    index=0,
)

# --- Chat Interface ---
st.header("Interactive Chatbot")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("How can I assist you today?"):
    # Append the prompt to the chat history
    st.session_state.chat_history.append({"role": USER, "content": prompt})
    with st.chat_message(USER):
        st.markdown(prompt)

    # Save the user's question to a text file
    with open("user_questions.txt", "a", encoding="utf-8") as file:
        file.write(f"Question user: {prompt}\n")

    with st.chat_message(ASSISTANT):
        if st.session_state.collection:
            metadatas, retrieved_data = [], ""
            if st.session_state.columns_to_answer:
                search_func = hyde_search if st.session_state.search_option == "Hyde Search" else vector_search
                model = st.session_state.llm_model if st.session_state.llm_type == ONLINE_LLM else None

                if st.session_state.search_option == "Vector Search":
                    metadatas, retrieved_data = vector_search(
                        st.session_state.embedding_model,
                        prompt,
                        st.session_state.collection,
                        st.session_state.columns_to_answer,
                        st.session_state.number_docs_retrieval
                    )

                    enhanced_prompt = """
                    N√™u c√¢u tr·∫£ l·ªùi sau kh√¥ng li√™n quan ƒë·∫øn tuy·ªÉn Sinh tr∆∞·ªùng ƒê·∫°i H·ªçc C√¥ng Ngh·ªá Th√¥ng Tin ƒê·∫°i H·ªçc Qu·ªëc GIa Th√†nh Ph·ªë HCM (UIT) th√¨ lich s·ª± t·ª´ ch·ªëi tr·∫£ l·ªùi, 
                    C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng l√†: "{}".Tr·∫£ l·ªùi n√≥ d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c truy xu·∫•t sau ƒë√¢y: \n{} """.format(prompt, retrieved_data)

                elif st.session_state.search_option == "Hyde Search":
                    if st.session_state.llm_type == ONLINE_LLM:
                        metadatas, retrieved_data = search_func(
                            model,
                            st.session_state.embedding_model,
                            prompt,
                            st.session_state.collection,
                            st.session_state.columns_to_answer,
                            st.session_state.number_docs_retrieval,
                            num_samples=1 if st.session_state.search_option == "Hyde Search" else None
                        )

                    enhanced_prompt = """
                    N√™u c√¢u tr·∫£ l·ªùi sau kh√¥ng li√™n quan ƒë·∫øn tuy·ªÉn Sinh tr∆∞·ªùng ƒê·∫°i H·ªçc C√¥ng Ngh·ªá Th√¥ng Tin ƒê·∫°i H·ªçc Qu·ªëc GIa Th√†nh Ph·ªë HCM (UIT) th√¨ lich s·ª± t·ª´ ch·ªëi tr·∫£ l·ªùi, 
                    C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng l√†: "{}".Tr·∫£ l·ªùi n√≥ d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c truy xu·∫•t sau ƒë√¢y: \n{} """.format(prompt, retrieved_data)

                if metadatas:
                    flattened_metadatas = [item for sublist in metadatas for item in sublist]
                    metadata_df = pd.DataFrame(flattened_metadatas)
                    st.sidebar.subheader("Retrieval data")
                    st.sidebar.dataframe(metadata_df)
                    st.sidebar.subheader("Full prompt for LLM")
                    st.sidebar.markdown(enhanced_prompt)
                else:
                    st.sidebar.write("No metadata to display.")

                if st.session_state.llm_model:
                    response = st.session_state.llm_model.generate_content(enhanced_prompt)
                    with open("user_questions.txt", "a", encoding="utf-8") as file:
                        file.write(f"Answer of Chatbot: {response}\n")
                    st.markdown(response)
                    st.session_state.chat_history.append({"role": ASSISTANT, "content": response})
            else:
                st.warning("Please select a model to run.")
        else:
            st.warning("Please select columns for the chatbot to answer from.")


