import streamlit as st
import pandas as pd
import os
from sentence_transformers import SentenceTransformer
import chromadb
import google.generativeai as genai
from IPython.display import Markdown
from search import vector_search, hyde_search
from llms.onlinellms import OnlineLLMs
import time
from constant import  USER, ASSISTANT, VIETNAMESE, ONLINE_LLM, GEMINI, DB
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import Document as langchainDocument
# --- Streamlit Configuration ---
st.set_page_config(page_title="UIT Admissions Chatbot", layout="wide" , page_icon="https://tuyensinh.uit.edu.vn/sites/default/files/uploads/images/uit_footer.png")
st.markdown("""
    <style>
        header {visibility: hidden;}
        footer {visibility: hidden;}
        /* Gi·∫£m kho·∫£ng tr·∫Øng ph√≠a tr√™n */
        .block-container {
            padding-top: 1rem;
        }
        [data-testid="stSidebarNav"] {display: none;}
    </style>
    """, unsafe_allow_html=True)

def clear_session_state():
    for key in st.session_state.keys():
        del st.session_state[key]

        
st.sidebar.header("ƒê√°nh gi√° Chatbot")
review = st.sidebar.text_area("Vi·∫øt ƒë√°nh gi√° c·ªßa b·∫°n t·∫°i ƒë√¢y:")

if st.sidebar.button("L∆∞u ƒë√°nh gi√°"):
    if review.strip():
        with open("user_reviews.txt", "a", encoding="utf-8") as review_file:
            review_file.write(f" Rview of user: {review}\n")
        st.sidebar.success("ƒê√°nh gi√° c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c l∆∞u!")
    else:
        st.sidebar.error("Vui l√≤ng nh·∫≠p n·ªôi dung ƒë√°nh gi√° tr∆∞·ªõc khi l∆∞u!")

st.markdown(
    """
    <h1 style='display: flex; align-items: center;'>
        <img src="https://tuyensinh.uit.edu.vn/sites/default/files/uploads/images/uit_footer.png" width="50" style='margin-right: 10px'>
        UIT Admissions Chatbot 
    </h1>
    """,
    unsafe_allow_html=True
)
st.markdown("Welcome to the UIT Admissions Chatbot!‚ùì‚ùì‚ùì Discover all the information you need about admissions, üìöprograms, üí∏scholarships, üåüStudent Life at UIT and more with us.")

if "language" not in st.session_state:
    st.session_state.language = VIETNAMESE

if "embedding_model" not in st.session_state:
    if st.session_state.language == VIETNAMESE:
        st.session_state.embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')
        st.session_state.embedding_model_name = 'keepitreal/vietnamese-sbert'
        st.success("Using Vietnamese embedding model: keepitreal/vietnamese-sbert")

# Initialize llm_model
if "llm_model" not in st.session_state:
    api_key = "AIzaSyBwvb8_AXyn_IZwn92WnqWQpKKM9sMfszQ"  
    st.session_state.llm_model = OnlineLLMs(name=GEMINI, api_key=api_key, model_version="learnlm-1.5-pro-experimental")#gemini-1.5-pro  #tunedModels/datanfinal-ae8czmglqym4
    st.session_state.api_key_saved = True
    print(" API Key saved successfully!")

# Initialize other session state variables
if "client" not in st.session_state:
    st.session_state.client = chromadb.PersistentClient("db")
if "search_option" not in st.session_state:
    st.session_state.search_option = "Vector Search"
if "open_dialog" not in st.session_state:
    st.session_state.open_dialog = None
if "source_data" not in st.session_state:
    st.session_state.source_data = "UPLOAD"
if "chunks_df" not in st.session_state:
    st.session_state.chunks_df = pd.DataFrame()
if "random_collection_name" not in st.session_state:
    st.session_state.random_collection_name = None

st.session_state.chunkOption = "SemanticChunker"
st.session_state.number_docs_retrieval = 15
st.session_state.llm_type = ONLINE_LLM
if "random_collection_name" in st.session_state and st.session_state.random_collection_name and not st.session_state.chunks_df.empty:
    st.session_state.columns_to_answer = [col for col in st.session_state.chunks_df.columns if col != "chunk"]
# T·ª± ƒë·ªông t·∫£i collection khi trang ƒë∆∞·ª£c load
if "collection" not in st.session_state:  # Ki·ªÉm tra n·∫øu collection ch∆∞a ƒë∆∞·ª£c load
    def load_func(collection_name):
        st.session_state.collection = st.session_state.client.get_collection(name=collection_name)
        st.session_state.random_collection_name = collection_name
        st.session_state.data_saved_success = True
        st.session_state.source_data = DB
        data = st.session_state.collection.get(include=["documents", "metadatas"])
        metadatas = data["metadatas"]
        column_names = []
        if metadatas and metadatas[0].keys():
            column_names.extend(metadatas[0].keys())
            column_names = list(set(column_names))
        st.session_state.chunks_df = pd.DataFrame(metadatas, columns=column_names)

    # G·ªçi load_func khi trang ƒë∆∞·ª£c load
    load_func("rag_collection-DataOfUIT")
    st.success("LOAD COLLECTION DONE!!")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("How can I assist you today?"):
    # Append the prompt to the chat history
    st.session_state.chat_history.append({"role": USER, "content": prompt})
    with st.chat_message(USER):
        st.markdown(prompt)

    # Save the user's question to a text file
    with open("user_questions.txt", "a", encoding="utf-8") as file:
        file.write(f"Question user: {prompt}\n")

    with st.chat_message(ASSISTANT):
        if st.session_state.collection:
            metadatas, retrieved_data = [], ""
            if st.session_state.columns_to_answer:
                search_func = hyde_search if st.session_state.search_option == "Hyde Search" else vector_search
                model = st.session_state.llm_model if st.session_state.llm_type == ONLINE_LLM else None

                if st.session_state.search_option == "Vector Search":
                    metadatas, retrieved_data = vector_search(
                        st.session_state.embedding_model,
                        prompt,
                        st.session_state.collection,
                        st.session_state.columns_to_answer,
                        st.session_state.number_docs_retrieval
                    )

                    enhanced_prompt = """
                    B·∫°n ƒëang ƒë√≥ng vai m·ªôt chuy√™n gia t∆∞ v·∫•n tuy·ªÉn sinh c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin (UIT) thu·ªôc ƒê·∫°i h·ªçc Qu·ªëc gia TP.HCM. 
                    Khi nh·∫≠n ƒë∆∞·ª£c c√¢u h·ªèi, h√£y b·∫Øt ƒë·∫ßu b·∫±ng l·ªùi ch√†o th√¢n thi·ªán v√† gi·ªõi thi·ªáu ng·∫Øn g·ªçn v·ªÅ vai tr√≤ c·ªßa b·∫°n. 
                    N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn tuy·ªÉn sinh c·ªßa UIT, h√£y l·ªãch s·ª± t·ª´ ch·ªëi v√† gi·∫£i th√≠ch r·∫±ng b·∫°n ch·ªâ h·ªó tr·ª£ c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn tuy·ªÉn sinh UIT. 
                    nƒÉm nay l√† nƒÉm 2025 c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng s·∫Ω ch·ªâ c√≥ trong nƒÉm 2024 ƒë·ªï l·∫°i th√¥i n·∫øu c√¢u h·ªèi v·ªÅ nƒÉm 2025 s·∫Ω ƒë∆∞a ra l√Ω do l√† 2025 ch∆∞a ƒë∆∞·ª£c tuy·ªÉn sinh
                    D·ª±a tr√™n d·ªØ li·ªáu ƒë√£ truy xu·∫•t b√™n d∆∞·ªõi, h√£y tr·∫£ l·ªùi theo d·∫°ng li·ªát k√™ m·ªôt c√°ch ch√≠nh x√°c, ng·∫Øn g·ªçn v√† th√¢n thi·ªán. 
                    C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng l√†: "{}"
                    D·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi l√†: \n{} """.format(prompt, retrieved_data)
                    
                if st.session_state.llm_model:
                    response = st.session_state.llm_model.generate_content(enhanced_prompt)
                    with open("user_questions.txt", "a", encoding="utf-8") as file:
                        file.write(f"Answer of Chatbot: {response}\n")
                    st.markdown(response)
                    st.session_state.chat_history.append({"role": ASSISTANT, "content": response})
            else:
                st.warning("Please select a model to run.")
        else:
            st.warning("Please select columns for the chatbot to answer from.")


